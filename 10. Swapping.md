# Swapping
---
> Swapping이란?  
> Physical Memory에 담기엔 너무 큰 Program들이 존재한다.  
> 그 Program들을 필요할 때마다 Memory에 올리고 필요 없을 때 Disk에 Evict하는 동작을 Swapping이라고 한다.  

## Memory Hierarchy
> Swapping을 위해선, Memory Hierarhcy에서 추가적인 Level이 필요하다.  
> 현대에서는 Hard disk가 이를 담당하고 있다.  

![image](https://user-images.githubusercontent.com/71700079/165473718-a2dea965-7096-4ed8-8ddf-876ccfa61a33.png)  
- 각 Layer은 상위 계층의 Backing Store 역할을 한다.  

## How to Swap?
- Overlays
  - 개발자가 Code Level에서 동작을 작성해 수동으로 직접 하는 방식이다.
  - Code 개발 단계에서 개발자가 주의 깊게 메모리 관리를 해야할 필요성이 있다.
- Process-Level
  - Process의 Address Space 전체를 Backing Store(Disk)로 보내는 방식이다.
  - I/O나 어떤 Event를 기다리는 경우에, Process를 통째로 Backing Store로 내려버린다.
- Page-Level
  - Backing store로 Page 단위로 내려보낸다. (Swap out)
  - 필요한 부분을 Demand하면, 그 때 다시 들고 올라온다. (Swap in)

## Where to swap?
> Disk의 특정 영역에 Swap Partition을 만들어서 저장한다.  
> 보통 Swap space의 한 Block은 Page size와 동일하다.  

![image](https://user-images.githubusercontent.com/71700079/165475161-f4a0ef90-03ea-448a-815e-16b627583b8e.png)  
- Swap Space도 실제 메모리는 아니지만, 가상의 공간처럼 사용이 가능하다.

### Present bit
> Valid bit와는 별개로, Physical Memory에 존재하는 지 여부를 나타내는 bit  

- 1 : Physical Memory에 올라와서 존재하는 Page이다.
- 0 : Backing Store에 존재하는 Page로 현재 Physical Memory에 존재하지 않는다.

### Page Fault
- Major Page Fault : 해당 Page가 Physical Memory로 올라와있지(Swap in) 않은 상태이다.
  - OS가 Disk에서 Memory로 들고 올라와야 한다.(Page Fault Handler)
  - TLB Miss가 났을 때, Page의 Present Bit 검사 단계 까지는 MMU(Hardware)이 처리 가능하다.
  - 하지만 이후에 Disk에서 Memory로 Swap in하거나 Replace하는 것은 OS가 해야하는 일이다.
- 만약 Memory가 가득찼다면?
  - 누군가를 쫓아내서(Page out) 공간을 만들고 필요한 Page를 Swap in 시켜야 한다.

## Page Replacement Policy
- 만약 OS가 Physical Memory가 가득찼을 때만 Swap in/out을 하면 어떨까?
  - 가득찰 때 까지 기다렸다가 해버리면, Page를 하나 Swap해도 다시 가득찬 상태이기 때문에 또 Swap을 해야 한다.
  - 하나의 Page씩 Disk에 작성하고 메모리로 가져오고 하는 경우엔 엄청난 Overhead가 발생한다.
- Swap Daemon(Page Daemon)
  - 위와 같은 이유로 가득찰 때까지 기다리지 않는다.
  - Low / High Watermark Page
    - Free Page의 크기가 Low Watermark보다 낮아지기 시작하면
    - 필요 없는 Page들을 Free Page의 크기가 High Watermark 만큼 남을 때 까지 싹 다 쫓아낸다.

## What to Swap?
> 그럼 모든 Page들은 Swap이 필요할까?  

- Kernel Code / Data / Stack : Not Swapped, 항상 Memory에 존재해야만 한다.
- Page Table : Not swapped.
- User code : Dropped, Swap하지 않고 그냥 삭제해도 된다.
  - Why?) Code는 원래 Disk에 존재하며, Runtime에 수정될 일이 없으므로 삭제해도 된다.
- User Data : Dropped or Swapped
  - Why?) Data는 원래 Disk에 존재해서 삭제해도 되지만, Runtime에 수정이 되는 경우(Global, Static)엔 Swap을 해주어야 한다.
- User Heap/Stack : Swapped, Disk에 원본이 존재하지 않으므로 무조건 Swap해야 한다.
- File Mapped to User process : Drop or File System
  - Why?) File System에 원본이 존재한다. 따라서 삭제해도 되지만, 수정이 되는 경우엔 File System에 다시 저장해야 한다.
- Page Cache Pages : Drop or File system, 위와 같은 이유이다.

## Replacement Policy
### Cache Management
> 우리는 Swap Partition에 대해서 Physical Memory를 Cache 처럼 사용하고 있다.  
> 그럼 어떤 Replacement Policy가 좋은 지 비교를 해야하는데, 그 지표가 무엇일까?  

- AMAT(Average Memory Access Time) : (Hit ratio * Memory Access Time) + (Miss ratio * Disk Access time)
  - Cache Miss가 최소가 되는 Replacement Policy가 당연히 가장 좋은 Policy일 것!

### Optimal Replacement Policy
- 가장 먼 미래에 사용될 예정인 Page를 쫓아내는 것이 최적의 방법이다!
  - 하지만 이 방법은 미래를 알아야 하는 방법이라서 이론으로만 존재한다.
  - 성능 비교를 위한 기준점으로만 사용한다.

### Simple Policy : FIFO
- Page를 일종의 Queue에 넣어서 관리한다.
- Memory에 먼저 올라온 순서대로 쫓아내고 Swap한다.
  - 먼저 올라왔다고 해서 중요하지 않은 Page라는 보장이 없어서 실제로 쓰이지 않는다!
- __Belady's Anomaly__
  - 보통은 Cache의 용량이 커지면, Hit ratio는 올라가기 마련이다.
  - 하지만 FIFO 방식(Queue 방식)을 사용하는 경우엔 이상하게 Miss ratio가 올라가버린다!

### Simple Policy : Random
- 아무 Page나 Random하게 골라서 쫓아낸다.
- 물론 최선의 경우에는 최적 Policy와 같은 성능을 낼 것이다.
  - 하지만 너무 도박이다.

### LRU
> 과거를 사용하는 대표적 방법 2가지중 하나인 LRU(Least Recently Used).  
> 다른 하나는 LFU(Least Frequent Used)가 있다.  

- 과거에 사용한지 가장 오래된 것을 쫓아낸다.
- Temporal Locality 때문에 Optimal에 거의 근사한 성능을 낼 수 있다.
  - 하지만 완벽한 LRU를 구현하기란, 현실에서 거의 불가능하다.
  - Why?) Page가 접근될 때마다 언제 접근했는지 기록을 해야한다. 이는 엄청난 Overhead를 동반한다.
- FIFO와 반대로 __Stack Algorithm__ 에 의해 동작한다.
  - 이는 곧 Belady's Anomaly가 발생하지 않음을 의미한다.
- Frequency는 고려하지 않으며, 모든 Workload에 잘 들어 맞는 방식이라고는 할 수 없다.

### Workload Example : No Locality  
> Locality가 전혀 없이, Random한 Memory 접근이 이루어 지는 경우엔 성능이 어떻게 될까?  
![image](https://user-images.githubusercontent.com/71700079/166262956-466f030c-8b36-472e-a485-5daf6f424d12.png)  
- LRU의 이점은 Temporal Locality에서 오는 것인데, 그 이점을 이용하지 못하는 경우이다.
  - 따라서 LRU, FIFO, RANDOM 모두 동일한 성능을 보인다.

### Workload Example : 80 - 20 Workload
> 80%의 Memory 참조가 Memory의 20% 범위 내에서 모두 이루어지는 경우엔 성능이 어떨까?  
![image](https://user-images.githubusercontent.com/71700079/166263014-d06cd5f8-8c65-483e-ab2f-846aca20a0a0.png)  
- 극한의 Locality를 보여주는 Case이다.
  - 이 경우엔 Locality를 이용하는 LRU가 당연하게도 가장 높은 성능을 보인다.
  - RANDOM과 FIFO는 동일한 성능을 보인다.
  - Cache 용량이 무한정 늘어나는 경우에, 왜 성능이 동일하게 수렴하게 될까?
    - Cache 용량이 늘어나는 경우엔 한 번에 다 집어넣고 모두 Hit가 나게 하면 되므로, 모두 동일한 성능으로 수렴한다.  

### Workload Example : Looping Sequential
> 0부터 N까지 순차적으로 순회하는 Loop의 경우엔 어떤 성능을 보일까?  
![image](https://user-images.githubusercontent.com/71700079/166263049-25c98fc6-f083-4b51-8854-2eb270f95ac6.png)   
- Graph를 보면, LRU가 특정 Cache 용량 까지는 바닥을 기다가 갑자기 훅 올라가게 됨을 알 수 있다. 이는 왜 그럴까?
  - Loop의 순회 횟수보다 Cache의 용량이 작은 경우, LRU는 모든 경우에 Miss가 나게 된다.
    - ex) 0..50 까지 Loop를 도는데 용량이 25인 경우, 0 ~ 25까지 채우면서 Miss가 나고 26부터는 모두 Replace 해야 하므로 싹 다 Miss가 난다.

### Implementing Historical Algorithm
- LRU를 완벽하게 구현하기 위해서는, Page 마다의 접근 시간, 접근 횟수 등을 모두 저장을 하고 있어야 한다.
  - 이는 엄청난 Overhead를 동반하므로, 완벽한 LRU는 구현하기 쉽지 않다.
  - 따라서 우리는 Hardware의 도움이 필요하다!
- Apporximating LRU
  - Hardware를 이용해서, Use bit만 1로 표시를 해준다. (접근했던 Page라는 의미로, Page Table에 저장한다.) 
  - Hardware는 이를 0으로 만드는 동작은 따로 하지 않는다. (이는 OS에게 책임을 돌린다.)

### Clock Alogrithm  
![image](https://user-images.githubusercontent.com/71700079/166264603-20e0bd38-c53b-4b63-8c0b-14aa82f9ff83.png)  
- 위와 같이 Page를 Circular List처럼 관리한다.
- 시계 바늘 돌듯이 List를 순회하며 Use bit가 0인 Page를 찾아서 Disk로 Evict 시킨다.
  - 하지만 이 Page가 사용한 지 가장 오래된 Page라고 보장할 수는 없다.
  - 다만 LRU와 거의 유사한 성능을 발휘한다.  
  ![image](https://user-images.githubusercontent.com/71700079/166264755-c7ec048e-bce6-4f8d-b2c7-32ee2609d001.png)  

### Considering Dirty Pages
- Memory에 올라온 이후로, 수정이 되는 Page는 Page Table의 Dirty Bit가 1로 바뀐다.
  - 이런 경우에는 삭제를 하지 않고 반드시 Disk에 Write back을 하고 Evict를 시켜야 하는 경우이다.
  - Dirty Bit가 0인 경우에는 바로 Free(삭제) 시킨다.

### Page Selection Policy
> Memory에 어떤 Page를 들고 올라오며, 언제 들고 올라올 것인지에 관한 Policy이다.  
- 실제로 필요할 때마다 Memory에 Page를 올리면, 성능에 별로 좋지 않은 영향을 끼친다.
- Prefetching : 성능 저하를 최대한 없이 동작하기 위해서 사용하는 방식.
  - 어떤 Page가 사용될 지 미리 파악을 한 후에 Memory로 미리 다 올려 놓는다.  
  ![image](https://user-images.githubusercontent.com/71700079/166265264-eb89bfad-446e-4c14-bdfa-21231d407c68.png)  
  - 예를 들어 Page 1을 올릴 때, Page 2도 쓸 가능성이 높다고 판단하여 동시에 올리는 것이다.
- Clustering(Grouping) : Prefetching을 좀 더 발전시킨 방법이다.
  - Data를 Read하거나 Write하는 경우엔, 하나하나 하기 보다는 덩어리(Cluster)로 한 번에 동작함이 유리하다.
  - 그래서 R/W를 동시에 진행할 Page Cluster을 미리 만들어 놓고 동시에 올렸다 내렸다 한다.

### Thrashing
> 과연 Memory를 너무 극한까지 사용해서 용량이 부족해진다면 어떤 일이 발생할까?  
- Swap Daemon 규칙에 따라 Memory 용량을 최대까지 쓰지는 않지만, 그것 마저도 막을 수 없을 만큼 Memory 용량이 부족하다면?  
  ![image](https://user-images.githubusercontent.com/71700079/166265869-d1f67167-0937-442b-8609-7a350c7c3ff5.png)  
  - 위 사진과 같이 CPU 사용률이 급격하게 떨어진다. 왜 이럴까?
    - 원래 CPU는 Process가 많아 질수록 CPU 사용량이 올라간다.
    - 하지만 Memory가 겉잡을 수 없이 용량이 부족해지는 경우, CPU는 전적으로 Replacement만 담당하게 되어버린다.
    - 그래서 CPU는 모든 Process를 중단하고 Replace만 하게 되는 순간이 오게 되는데, 이를 Thrashing이라고 한다.
